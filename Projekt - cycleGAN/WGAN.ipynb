{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo25BZzDbQ-Q",
        "colab_type": "text"
      },
      "source": [
        "# WGAN for creating cartoon images out of celebrity images\n",
        "### Authors: Memeti Nurdzane, WrÃ³bel Anna\n",
        "Aim of this project is to translate cartoon style to celebrity images.\n",
        "It uses celebrity dataset (aligned and cropped): celbA and cartoon10k dataset (https://google.github.io/cartoonset/download.html)\n",
        "Information about image attributes was not used. In this part we try to implement and tune WGAN version of cyclegan.\n",
        "\n",
        "The difference from cycleGAN implementation is in:\n",
        "- the activation function of last layer of the discriminator to linear,\n",
        "- implementing Wasserstein loss for generator and discriminator,\n",
        "- clipped weights (Lipschitz contraint)\n",
        "- use of RMSprop optimizers with no momentum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoL23u8sp7lL",
        "colab_type": "text"
      },
      "source": [
        "### Prepare environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY85F8AjQMTO",
        "colab_type": "text"
      },
      "source": [
        "Mount drive and change directories.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01aJN2pmXKhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "# drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/Colab/ANN/\n",
        "!ls | head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA7bpeTaQNFB",
        "colab_type": "text"
      },
      "source": [
        "Import tensorflow and check if GPU is available.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4Zc5K3KPrV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
        "print('GPU device name: ' + tf.test.gpu_device_name())\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQM5HJQbckpD",
        "colab_type": "text"
      },
      "source": [
        "Import libraries needed for the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIjY5QRJdsIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sid7HfTNp_HV",
        "colab_type": "text"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ1WZE3lbp8c",
        "colab_type": "text"
      },
      "source": [
        "Create datasets as tf datasets together with image preprocessing:\n",
        "- random jitter: the image is resized to 286 x 286 and then randomly cropped to 256 x 256\n",
        "- random mirroring, the image is randomly flipped horizontally \n",
        "- normalization to range -1, 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGzjF7DZcyMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "def load(filelist):\n",
        "  assert len(filelist) > 0\n",
        "  filenames = tf.constant(filelist)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((filenames))\n",
        "  def _parse_function(filename):\n",
        "      image_string = tf.io.read_file(filename)\n",
        "      image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "      image_resized = tf.image.resize(image_decoded, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "      cropped_image = tf.image.random_crop(image_resized, size=[256, 256, 3])\n",
        "      flipped_image = tf.image.random_flip_left_right(cropped_image)\n",
        "      casted_image = tf.cast(flipped_image, tf.float32)\n",
        "      normalized_image = (casted_image / 127.5) - 1\n",
        "\n",
        "      return normalized_image\n",
        "  return dataset.map(_parse_function)\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "n =1000      #number of train images in one dataset\n",
        "k = 100      #number of test images in one dataset\n",
        "\n",
        "#Create filelist and cartoon train (n images) and test set (k images)\n",
        "filelist = glob.glob('/content/drive/My Drive/Colab/ANN/cartoons/*.jpg')\n",
        "cartoons_train = load(filelist[0:n]).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "cartoons_test = load(filelist[n:n+k]).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "#Create filelist#Create filelist and cartoon train (n images) and test set (k images)\n",
        "filelist = glob.glob('/content/drive/My Drive/Colab/ANN/celebrities/*.jpg')\n",
        "celebrities_train = load(filelist[0:n]).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "celebrities_test = load(filelist[n:n+k]).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2UAgIz9pyib",
        "colab_type": "text"
      },
      "source": [
        "Check sample images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smtd0PFn1q1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample_cartoon = next(iter(cartoons_train))\n",
        "sample_celebrity = next(iter(celebrities_train))\n",
        "sample_cartoon_test = next(iter(cartoons_test))\n",
        "sample_celebrity_test = next(iter(celebrities_test))\n",
        "\n",
        "# plt.title('Cartoon')\n",
        "# plt.imshow(sample_cartoon[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.title('Celebrity')\n",
        "plt.imshow(sample_celebrity[0] * 0.5 + 0.5)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVETLWBkp3ke",
        "colab_type": "text"
      },
      "source": [
        "## Build network and define loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9n99NiBy_vh",
        "colab_type": "text"
      },
      "source": [
        "Define modified unet network based on pix2pix architecture from tensorflow examples (https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) adjusted to the needs of the cycleGAN for our purpose. For the cycleGAN instance normalization will be used. \n",
        "For the WGAN implementation activation function in discriminators last layer is changed to linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grNdcAIa43ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InstanceNormalization(tf.keras.layers.Layer):\n",
        "  \"\"\"Instance Normalization Layer.\"\"\"\n",
        "\n",
        "  def __init__(self, epsilon=1e-5):\n",
        "    super(InstanceNormalization, self).__init__()\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.scale = self.add_weight(\n",
        "        name='scale',\n",
        "        shape=input_shape[-1:],\n",
        "        initializer=tf.random_normal_initializer(1., 0.02),\n",
        "        trainable=True)\n",
        "\n",
        "    self.offset = self.add_weight(\n",
        "        name='offset',\n",
        "        shape=input_shape[-1:],\n",
        "        initializer='zeros',\n",
        "        trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
        "    inv = tf.math.rsqrt(variance + self.epsilon)\n",
        "    normalized = (x - mean) * inv\n",
        "    return self.scale * normalized + self.offset\n",
        "\n",
        "def downsample(filters, size, apply_norm=True):\n",
        "  \"\"\"Downsamples an input.\n",
        "  Conv2D => Instancenorm => LeakyRelu\n",
        "  Args:\n",
        "    filters: number of filters\n",
        "    size: filter size,\n",
        "    apply_norm: If True, adds the instance norm layer\n",
        "  Returns: Downsample Sequential Model\n",
        "  \"\"\"\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_norm:\n",
        "      result.add(InstanceNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "  \"\"\"Upsamples an input.\n",
        "  Conv2DTranspose => Instancenorm => Dropout => Relu\n",
        "  Args:\n",
        "    filters: number of filters\n",
        "    size: filter size\n",
        "    apply_dropout: If True, adds the dropout layer\n",
        "  Returns: Upsample Sequential Model\n",
        "  \"\"\"\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "  \n",
        "  result.add(InstanceNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "    result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result\n",
        "\n",
        "def unet_generator(output_channels):\n",
        "  \"\"\"Modified u-net generator model.\n",
        "  Returns: Generator model\n",
        "  \"\"\"\n",
        "\n",
        "  down_stack = [\n",
        "      downsample(64, 4, apply_norm=False),\n",
        "      downsample(128, 4),\n",
        "      downsample(256, 4),\n",
        "      downsample(512, 4),\n",
        "      downsample(512, 4),\n",
        "      downsample(512, 4),\n",
        "      downsample(512, 4),\n",
        "      downsample(512, 4),\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "      upsample(512, 4, apply_dropout=True),\n",
        "      upsample(512, 4, apply_dropout=True),\n",
        "      upsample(512, 4, apply_dropout=True),\n",
        "      upsample(512, 4),\n",
        "      upsample(256, 4),\n",
        "      upsample(128, 4),\n",
        "      upsample(64, 4),\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      output_channels, 4, strides=2,\n",
        "      padding='same', kernel_initializer=initializer,\n",
        "      activation='tanh')  \n",
        "\n",
        "  # concat = tf.keras.layers.Concatenate()\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    # x = concat([x, skip])\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "\n",
        "def discriminator():\n",
        "  \"\"\"PatchGan discriminator model\n",
        "  \"\"\"\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "  x = inp\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x) \n",
        "  down2 = downsample(128, 4)(down1) \n",
        "  down3 = downsample(256, 4)(down2)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)\n",
        "  conv = tf.keras.layers.Conv2D(\n",
        "      512, 4, strides=1, kernel_initializer=initializer,\n",
        "      use_bias=False)(zero_pad1) \n",
        "\n",
        "  norm1 = InstanceNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(\n",
        "      1, 4, strides=1,\n",
        "      kernel_initializer=initializer, activation='linear')(zero_pad2)\n",
        "\n",
        "  return tf.keras.Model(inputs=inp, outputs=last)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTtzQYLZdYTT",
        "colab_type": "text"
      },
      "source": [
        "Create generators and discrimnators with defined above modified unet structure and instance normalization. Wasserstein loss will is used to determine genrators and discriminators losses. During training weight clipping will be applied to limit weights to the range defined by the threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXs-2z2Wey2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "# Generators\n",
        "generator_AB = unet_generator(OUTPUT_CHANNELS)\n",
        "generator_BA = unet_generator(OUTPUT_CHANNELS)\n",
        "\n",
        "# Discriminators\n",
        "discriminator_A = discriminator()\n",
        "discriminator_B = discriminator()\n",
        "\n",
        "LAMBDA = 10\n",
        "\n",
        "# Define Wasserstein loss\n",
        "from keras import backend\n",
        "def wasserstein_loss(true, pred):\n",
        "\n",
        "  return backend.mean(true * pred)\n",
        "\n",
        "# Define discrimnator loss \n",
        "def discriminator_loss(real, generated):\n",
        "  real_loss = wasserstein_loss(-tf.ones_like(real), real)\n",
        "\n",
        "  generated_loss = wasserstein_loss(tf.ones_like(generated), generated)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss * 0.5\n",
        "\n",
        "# Define generator loss\n",
        "def generator_loss(generated):\n",
        "  return wasserstein_loss(tf.ones_like(generated), generated)\n",
        "\n",
        "# Define cycle loss\n",
        "def calc_cycle_loss(real_image, cycled_image):\n",
        "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "  \n",
        "  return LAMBDA * loss1\n",
        "\n",
        "#Define identity loss\n",
        "def identity_loss(real_image, same_image):\n",
        "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "  return LAMBDA * 0.5 * loss\n",
        "\n",
        "lr1 = 0.00005\n",
        "lr2 = 0.00001\n",
        "generator_AB_optimizer = tf.keras.optimizers.RMSprop(lr=lr2)  #slower generator\n",
        "generator_BA_optimizer = tf.keras.optimizers.RMSprop(lr=lr2)\n",
        "discriminator_A_optimizer = tf.keras.optimizers.RMSprop(lr=lr1) #faster discriminator\n",
        "discriminator_B_optimizer = tf.keras.optimizers.RMSprop(lr=lr1)\n",
        "\n",
        "# weights clipping for Lipschitz constraint\n",
        "# threshold = 0.1\n",
        "threshold = 0.01\n",
        "def update_weights(model):\n",
        "  for l in model.layers:\n",
        "    weights = l.get_weights()\n",
        "    weights = [np.clip(w,-threshold ,threshold) for w in weights]\n",
        "    l.set_weights(weights)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHLYdPmn1tQo",
        "colab_type": "text"
      },
      "source": [
        "### Model structure\n",
        "Check the structure of generators and discriminatores\n",
        "Both generators have the same structure so it is enough to check only one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxJcUsgm1vbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator AB structure\n",
        "tf.keras.utils.plot_model(generator_AB, show_shapes=True, dpi=64)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zsyjqBKM77O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generator AB summary\n",
        "generator_AB.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1cfqIu_3vLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discriminator A structure\n",
        "tf.keras.utils.plot_model(discriminator_A, show_shapes=True, dpi=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQpSagMfNJJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discriminator A summary\n",
        "discriminator_A.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lGhT1eHrOrU",
        "colab_type": "text"
      },
      "source": [
        "Saving models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClMRbH0urNWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save checpoint for results analysis and retraining from a certain point\n",
        "checkpoint_path = \"./checkpoints2/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator_AB=generator_AB,\n",
        "                           generator_BA=generator_BA,\n",
        "                           discriminator_A=discriminator_A,\n",
        "                           discriminator_B=discriminator_B,\n",
        "                           generator_AB_optimizer=generator_AB_optimizer,\n",
        "                           generator_BA_optimizer=generator_BA_optimizer,\n",
        "                           discriminator_A_optimizer=discriminator_A_optimizer,\n",
        "                           discriminator_B_optimizer=discriminator_B_optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGtRo2DYf5y_",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Define training steps and function fro generating and plotting images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lt8XIw0f7Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot image with its prediction and save it as .png file\n",
        "def generate_images(model, test_input, fname):\n",
        "  prediction = model(test_input)\n",
        "  \n",
        "  plt.figure(figsize=(12, 12))\n",
        "\n",
        "  display_list = [test_input[0], prediction[0]]\n",
        "  title = ['Input Image', 'Predicted Image']\n",
        "\n",
        "  for i in range(2):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  # save figure\n",
        "  plt.savefig('/content/drive/My Drive/Colab/ANN/results_final/' + fname + '.png')\n",
        "  plt.show()\n",
        "\n",
        "# Define trainign step\n",
        "@tf.function\n",
        "def train_step(real_A, real_B):\n",
        "  # persistent is set to True because the tape is used more than\n",
        "  # once to calculate the gradients.\n",
        "  # A stands for celebrity\n",
        "  # B stands for cartoon\n",
        "\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # Generator AB translates A(celebrity) -> B(cartoon)\n",
        "    # Generator BA translates B -> A\n",
        "    \n",
        "    fake_B = generator_AB(real_A, training=True)\n",
        "    cycled_A = generator_BA(fake_B, training=True)\n",
        "\n",
        "    fake_A = generator_BA(real_B, training=True)\n",
        "    cycled_B = generator_AB(fake_A, training=True)\n",
        "\n",
        "    # same_x and same_y are used for identity loss.\n",
        "    same_A = generator_BA(real_A, training=True)\n",
        "    same_B = generator_BA(real_B, training=True)\n",
        "\n",
        "    disc_real_A = discriminator_A(real_A, training=True)\n",
        "    disc_real_B = discriminator_B(real_B, training=True)\n",
        "\n",
        "    disc_fake_A = discriminator_A(fake_A, training=True)\n",
        "    disc_fake_B = discriminator_B(fake_B, training=True)\n",
        "\n",
        "    # calculate the loss\n",
        "    gen_AB_loss = generator_loss(disc_fake_B) # adversarial\n",
        "    gen_BA_loss = generator_loss(disc_fake_A) # adversarial\n",
        "    \n",
        "    total_cycle_loss = calc_cycle_loss(real_A, cycled_A) + calc_cycle_loss(real_B, cycled_B)\n",
        "    \n",
        "    # Total generator loss = adversarial loss + cycle loss + identity\n",
        "    total_gen_AB_loss = gen_AB_loss + total_cycle_loss + identity_loss(real_B, same_B)\n",
        "    total_gen_BA_loss = gen_BA_loss + total_cycle_loss + identity_loss(real_A, same_A)\n",
        "\n",
        "    disc_A_loss = discriminator_loss(disc_real_A, disc_fake_A)\n",
        "    disc_B_loss = discriminator_loss(disc_real_B, disc_fake_B)\n",
        "  \n",
        "  # Calculate the gradients for generators and discriminators\n",
        "  generator_AB_gradients = tape.gradient(total_gen_AB_loss, \n",
        "                                        generator_AB.trainable_variables)\n",
        "  generator_BA_gradients = tape.gradient(total_gen_BA_loss, \n",
        "                                        generator_BA.trainable_variables)\n",
        "  \n",
        "  discriminator_A_gradients = tape.gradient(disc_A_loss, \n",
        "                                            discriminator_A.trainable_variables)\n",
        "  discriminator_B_gradients = tape.gradient(disc_B_loss, \n",
        "                                            discriminator_B.trainable_variables)\n",
        "  \n",
        "  # Apply the gradients to the optimizer\n",
        "  generator_AB_optimizer.apply_gradients(zip(generator_AB_gradients, \n",
        "                                            generator_AB.trainable_variables))\n",
        "\n",
        "  generator_BA_optimizer.apply_gradients(zip(generator_BA_gradients, \n",
        "                                            generator_BA.trainable_variables))\n",
        "  \n",
        "  discriminator_A_optimizer.apply_gradients(zip(discriminator_A_gradients,\n",
        "                                                discriminator_A.trainable_variables))\n",
        "  \n",
        "  discriminator_B_optimizer.apply_gradients(zip(discriminator_B_gradients,\n",
        "                                                discriminator_B.trainable_variables))\n",
        "\n",
        "  return total_gen_AB_loss, total_gen_BA_loss, disc_A_loss, disc_A_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw4AwdDxgI_o",
        "colab_type": "text"
      },
      "source": [
        "Training is performed for a defined number of EPOCHS with monitoring progres on chosen sample images and saving loss functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa_Bm05egJQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if does not exist create folder for storing results\n",
        "!mkdir -p '/content/drive/My Drive/Colab/ANN/results_final'\n",
        "\n",
        "# pick two images for training progress monitoring\n",
        "sample_celeb = next(iter(celebrities_train))\n",
        "sample_cart = next(iter(cartoons_train))\n",
        "\n",
        "# initialize arrays for generators and discriminators losses\n",
        "gen_AB_loss_all = []\n",
        "gen_BA_loss_all = []\n",
        "disc_A_loss_all = []\n",
        "disc_B_loss_all = []\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  n = 0\n",
        "  for image_A, image_B in tf.data.Dataset.zip((celebrities_train, cartoons_train)):\n",
        "\n",
        "    total_gen_AB_loss, total_gen_BA_loss, disc_A_loss, disc_B_loss = (\n",
        "        train_step(image_A, image_B))\n",
        "    \n",
        "    update_weights(discriminator_A)\n",
        "    update_weights(discriminator_B)\n",
        "    \n",
        "    gen_AB_loss_all.append(total_gen_AB_loss)\n",
        "    gen_BA_loss_all.append(total_gen_BA_loss)\n",
        "    disc_A_loss_all.append(disc_A_loss)\n",
        "    disc_B_loss_all.append(disc_B_loss)\n",
        "\n",
        "    if n % 5 == 0:\n",
        "      print ('.', end='')\n",
        "    n+=1\n",
        "\n",
        "  # Using a consistent image (sample_celeb and sample_cart ) to monitor \n",
        "  # training progress\n",
        "  generate_images(generator_AB, sample_celeb, 'train_celeb_' + str(epoch))\n",
        "  generate_images(generator_BA, sample_cart, 'train_cart' + str(epoch))\n",
        "\n",
        "  # save every 10th checkpoint\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "\n",
        "  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                      time.time()-start))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRLA3Gpcw7mq",
        "colab_type": "text"
      },
      "source": [
        "## Check model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHjoQ273N2H",
        "colab_type": "text"
      },
      "source": [
        "Plot the loss functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1MFUB0f3Qd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(gen_AB_loss_all, 'y', label = 'gen_AB_loss')\n",
        "plt.plot(gen_BA_loss_all, 'b', label = 'gen_BA_loss')\n",
        "plt.plot(disc_A_loss_all, 'r', label = 'disc_A_loss')\n",
        "plt.plot(disc_B_loss_all, 'g', label = 'disc_B_loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig('/content/drive/My Drive/Colab/ANN/results_final/losses.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBSm1Jijgt5A",
        "colab_type": "text"
      },
      "source": [
        "Generate images using test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXEpvfpSgw-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the trained model on the test dataset\n",
        "i = 1\n",
        "for inp in celebrities_test.take(3):\n",
        "  generate_images(generator_AB, inp, 'celeb_to_cartoon_test_' + str(i))\n",
        "  i+=1\n",
        "\n",
        "i = 1\n",
        "for inp in cartoons_test.take(3):\n",
        "  generate_images(generator_BA, inp, 'cartoon_to_celeb_test_' + str(i))\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}